{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 19:17:49.846999: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-05 19:17:49.889437: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-05 19:17:49.889497: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-05 19:17:49.889535: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-05 19:17:49.898771: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-05 19:17:49.899340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-05 19:17:50.994285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io\n",
    "from tensorflow import keras\n",
    "import tensorboard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import tweets_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../../Datasets/disaster_tweets\"\n",
    "CHECKPOINT_DIR = \"./tweets_classifier/checkpoints\"\n",
    "BATCH = 32\n",
    "SHUFFLE_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preproc: bool = 1):\n",
    "    # load preprocessed data from parquet\n",
    "    if preproc:\n",
    "        train_data = pd.read_parquet(f\"{DATA_DIR}/train_preprocessed.parquet\")\n",
    "        test_data = pd.read_parquet(f\"{DATA_DIR}/test_preprocessed.parquet\")\n",
    "    else:\n",
    "    # load unpreprocessed data from csvs\n",
    "        train_data = [pd.read_csv(f\"{DATA_DIR}/train.csv\", index_col = 0), pd.read_csv(f\"{DATA_DIR}/train2.csv\")[[\"keyword\",\"location\",\"text\",\"choose_one\"]]]\n",
    "        test_data = pd.read_csv(f\"{DATA_DIR}/test.csv\", index_col = 0)\n",
    "    return train_data, test_data\n",
    "\n",
    "def preprocess_and_save():\n",
    "    train_data, test_data = load_data(preproc = 0)\n",
    "    \n",
    "    train_data[1].rename(columns = {\"choose_one\": \"target\"}, inplace = True)\n",
    "    train_data[1][\"target\"] = (train_data[1][\"target\"] == \"Relevant\").astype(\"int\")\n",
    "\n",
    "    train_data = pd.concat([train_data[0], train_data[1]], axis = 0)\n",
    "\n",
    "    train_data.fillna(\"0\", inplace = True)\n",
    "    test_data.fillna(\"0\", inplace = True)\n",
    "\n",
    "    train_data.to_parquet(f\"{DATA_DIR}/train_preprocessed.parquet\")\n",
    "    test_data.to_parquet(f\"{DATA_DIR}/test_preprocessed.parquet\")\n",
    "\n",
    "    return train_data, test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = preprocess_and_save()\n",
    "# train_data, test_data = load_data(preproc = 0)\n",
    "train_data, test_data = load_data(preproc = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tf = tf.data.Dataset.from_tensor_slices((train_data[\"keyword\"] + train_data[\"location\"] + train_data[\"text\"], train_data[\"target\"])).shuffle(SHUFFLE_SEED).batch(BATCH)\n",
    "test_data_tf = tf.data.Dataset.from_tensor_slices((test_data[\"keyword\"] + test_data[\"location\"] + test_data[\"text\"])).batch(BATCH)\n",
    "\n",
    "text_vect = keras.layers.TextVectorization(\n",
    "    max_tokens = 20_000,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = 165\n",
    ")\n",
    "text_vect.adapt(train_data_tf.map(lambda txt, trgt: txt))\n",
    "\n",
    "train_data_tf_vec = train_data_tf.map(lambda txt, trgt: (text_vect(txt), trgt), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "test_data_tf_vec = test_data_tf.map(lambda txt: text_vect(txt), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "val_size = int(0.2 * len(train_data_tf_vec))\n",
    "\n",
    "validation = train_data_tf_vec.take(val_size)\n",
    "train = train_data_tf_vec.skip(val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define callbacks and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(f\"{CHECKPOINT_DIR}/tweets_classifier.tf\", save_best_only=True)\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(patience = 4)\n",
    "\n",
    "history = tweets_classifier.model.model.fit(train, epochs = 120, validation_data = validation, callbacks = [tensorboard_callback, checkpoint_callback, early_stopping_callback])\n",
    "model.save_weights(\"tweets_classifier/weights/weights.tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f01fc572560>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tweets_classifier.model.model\n",
    "model.load_weights(\"tweets_classifier/weights/weights.tf\")\n",
    "# model.evaluate(validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
